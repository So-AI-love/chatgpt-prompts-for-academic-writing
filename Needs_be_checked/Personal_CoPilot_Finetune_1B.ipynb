{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/So-AI-love/chatgpt-prompts-for-academic-writing/blob/main/Needs_be_checked/Personal_CoPilot_Finetune_1B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30_PXCJrB1ve",
        "outputId": "5d07e9ab-e021-48b0-8432-5f2fd5ec8849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Aug 30 10:07:43 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8MG0g-eEW9X",
        "outputId": "4e12466f-f007-4ceb-d233-76470f4fbebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DHS-LLM-Workshop'...\n",
            "remote: Enumerating objects: 810, done.\u001b[K\n",
            "remote: Counting objects: 100% (116/116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 810 (delta 81), reused 76 (delta 50), pack-reused 694\u001b[K\n",
            "Receiving objects: 100% (810/810), 45.52 MiB | 6.27 MiB/s, done.\n",
            "Resolving deltas: 100% (396/396), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/pacman100/DHS-LLM-Workshop.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqWoeV0zJF8E",
        "outputId": "62edf217-db8a-42a9-bb16-70b5d3f273c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.1)\n",
            "\u001b[33mWARNING: Skipping ninja as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1\n",
            "1.11.1.git.kitware.jobserver-1\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "!pip install packaging\n",
        "!pip uninstall -y ninja && pip install ninja\n",
        "!ninja --version\n",
        "!echo $?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvjHmB_pIwWA",
        "outputId": "e00541ce-3898-41cf-b353-0f1f5b46e24b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DHS-LLM-Workshop\n",
            "Already up to date.\n",
            "Collecting git+https://github.com/huggingface/transformers (from -r requirements.txt (line 1))\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-5cnyx7dj\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-5cnyx7dj\n",
            "  Resolved https://github.com/huggingface/transformers to commit 1bf2f36daf6731f001ea88ae53ba96acfb6c8497\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/huggingface/accelerate (from -r requirements.txt (line 2))\n",
            "  Cloning https://github.com/huggingface/accelerate to /tmp/pip-req-build-9z6vmbru\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate /tmp/pip-req-build-9z6vmbru\n",
            "  Resolved https://github.com/huggingface/accelerate to commit 0b5ac0253ec7fb795f2ddab47ce94b2881719b4c\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/huggingface/peft (from -r requirements.txt (line 3))\n",
            "  Cloning https://github.com/huggingface/peft to /tmp/pip-req-build-7qz4as92\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-req-build-7qz4as92\n",
            "  Resolved https://github.com/huggingface/peft to commit 0b2f950cc212dd8acda983760c0aba2bf4872a00\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting trl (from -r requirements.txt (line 4))\n",
            "  Downloading trl-0.7.0-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.9/117.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub (from -r requirements.txt (line 5))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate (from -r requirements.txt (line 6))\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets (from -r requirements.txt (line 7))\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes (from -r requirements.txt (line 8))\n",
            "  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops (from -r requirements.txt (line 9))\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb (from -r requirements.txt (line 10))\n",
            "  Downloading wandb-0.15.9-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken (from -r requirements.txt (line 11))\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (1.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (3.7.1)\n",
            "Collecting easyllm (from -r requirements.txt (line 16))\n",
            "  Downloading easyllm-0.4.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0->-r requirements.txt (line 1)) (3.12.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0->-r requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0->-r requirements.txt (line 1)) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0->-r requirements.txt (line 1)) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.0.dev0->-r requirements.txt (line 1))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.33.0.dev0->-r requirements.txt (line 1))\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0->-r requirements.txt (line 1)) (4.66.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.23.0.dev0->-r requirements.txt (line 2)) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.23.0.dev0->-r requirements.txt (line 2)) (2.0.1+cu118)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->-r requirements.txt (line 5)) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->-r requirements.txt (line 5)) (4.7.1)\n",
            "Collecting dill (from evaluate->-r requirements.txt (line 6))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from evaluate->-r requirements.txt (line 6))\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from evaluate->-r requirements.txt (line 6))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19 (from evaluate->-r requirements.txt (line 6))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 7)) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 7)) (3.8.5)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 10)) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 10))\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 10))\n",
            "  Downloading sentry_sdk-1.30.0-py2.py3-none-any.whl (218 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.8/218.8 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 10))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb->-r requirements.txt (line 10))\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb->-r requirements.txt (line 10))\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 10)) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 10)) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 10)) (3.20.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 12)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 12)) (2023.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 15)) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 15)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 15)) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 15)) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 15)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 15)) (3.1.1)\n",
            "Collecting nanoid==2.0.0 (from easyllm->-r requirements.txt (line 16))\n",
            "  Downloading nanoid-2.0.0-py3-none-any.whl (5.8 kB)\n",
            "Collecting pydantic==2.1.1 (from easyllm->-r requirements.txt (line 16))\n",
            "  Downloading pydantic-2.1.1-py3-none-any.whl (370 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m370.9/370.9 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.1.1->easyllm->-r requirements.txt (line 16)) (0.5.0)\n",
            "Collecting pydantic-core==2.4.0 (from pydantic==2.1.1->easyllm->-r requirements.txt (line 16))\n",
            "  Downloading pydantic_core-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 10)) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.3.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 10))\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.0.dev0->-r requirements.txt (line 1)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.0.dev0->-r requirements.txt (line 1)) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.0.dev0->-r requirements.txt (line 1)) (2023.7.22)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.23.0.dev0->-r requirements.txt (line 2)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.23.0.dev0->-r requirements.txt (line 2)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.23.0.dev0->-r requirements.txt (line 2)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.23.0.dev0->-r requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.23.0.dev0->-r requirements.txt (line 2)) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.23.0.dev0->-r requirements.txt (line 2)) (16.0.6)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 10))\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.23.0.dev0->-r requirements.txt (line 2)) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.23.0.dev0->-r requirements.txt (line 2)) (1.3.0)\n",
            "Building wheels for collected packages: transformers, accelerate, peft, pathtools\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.33.0.dev0-py3-none-any.whl size=7604296 sha256=6ae5df27ee2d648126846270de63ac6f97dbe352981bdbe46a484f6d42162c69\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f2bz674j/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
            "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for accelerate: filename=accelerate-0.23.0.dev0-py3-none-any.whl size=255798 sha256=58456eb5a4abb313af94c8afd9d39c6f5a11d26761a2ddd49e1ea5b0229deb89\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f2bz674j/wheels/f6/c7/9d/1b8a5ca8353d9307733bc719107acb67acdc95063bba749f26\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for peft: filename=peft-0.6.0.dev0-py3-none-any.whl size=106757 sha256=735458d8af3d81a3dd7a2a1a8c90db09a24be8ad48f1fc33448465f03814eda8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f2bz674j/wheels/4c/16/67/1002a2d4daa822eff130e6d85b90051b75d2ce0d26b9448e4a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=614003a3e21befa674a7ef71059e9a07ca45a770e6dbbdd7ff5069eebb194055\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built transformers accelerate peft pathtools\n",
            "Installing collected packages: tokenizers, safetensors, pathtools, nanoid, bitsandbytes, xxhash, smmap, setproctitle, sentry-sdk, pydantic-core, einops, docker-pycreds, dill, tiktoken, responses, pydantic, multiprocess, huggingface-hub, gitdb, transformers, GitPython, easyllm, wandb, datasets, evaluate, accelerate, trl, peft\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.6.1\n",
            "    Uninstalling pydantic_core-2.6.1:\n",
            "      Successfully uninstalled pydantic_core-2.6.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.2.1\n",
            "    Uninstalling pydantic-2.2.1:\n",
            "      Successfully uninstalled pydantic-2.2.1\n",
            "Successfully installed GitPython-3.1.32 accelerate-0.23.0.dev0 bitsandbytes-0.41.1 datasets-2.14.4 dill-0.3.7 docker-pycreds-0.4.0 easyllm-0.4.0 einops-0.6.1 evaluate-0.4.0 gitdb-4.0.10 huggingface-hub-0.16.4 multiprocess-0.70.15 nanoid-2.0.0 pathtools-0.1.2 peft-0.6.0.dev0 pydantic-2.1.1 pydantic-core-2.4.0 responses-0.18.0 safetensors-0.3.3 sentry-sdk-1.30.0 setproctitle-1.3.2 smmap-5.0.0 tiktoken-0.4.0 tokenizers-0.13.3 transformers-4.33.0.dev0 trl-0.7.0 wandb-0.15.9 xxhash-3.3.0\n"
          ]
        }
      ],
      "source": [
        "%cd /content/DHS-LLM-Workshop\n",
        "!git pull\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mf1YxDg9Effq",
        "outputId": "04aada6c-f1fb-4d54-ff33-dd901a428c93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.1.1.tar.gz (2.3 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.3 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.0.1+cu118)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn) (23.1)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from flash-attn) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->flash-attn) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->flash-attn) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.1.1-cp310-cp310-linux_x86_64.whl size=128602002 sha256=5c1e36839383a503761a4b97fc40898ebe4ec016547091daa5a8fd787ad342d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/66/ed/1ca5cd61374c9d9352fd9d49c3f0ea740e36ec6ab508b31b55\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUq-nUxnRtTC",
        "outputId": "6dae1cb8-d655-4eed-b14d-72524959655d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!wandb login --relogin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "dcbe7f239f7c43f7a894ffe6f7abd54f",
            "6bf0c013227e4aaaa8ec4eb93eeaf845",
            "1d1551729b8b4a6b9d07e3fe52bb3447",
            "3399240402c2453494d8378dfca965e7",
            "bf79f76aa37549cd91637532813750ee",
            "2631660841724a7aa1281c91af705267",
            "9b3f10fd6c804e4981fdef7072c892bc",
            "3098dd6d5bbc48c5ba618de25cd53ce0",
            "b25545fe76fc41988dc7908b366a7214",
            "963d4746ccd84d24be0a7928ef70fea3",
            "19c5bab192004bd4a3cb068a1c7fd8ea",
            "25f80e20899f42b89523a7a12f5ae3ea",
            "ff012677cb634ed5afdcb6a864af172b",
            "076bcade218645258f75c862699dcf66"
          ]
        },
        "id": "se069Y2AR9xn",
        "outputId": "7901be89-22f3-4211-8139-976ccfe8acba"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dcbe7f239f7c43f7a894ffe6f7abd54f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDuL0hrjR_jB",
        "outputId": "5c4cc687-e716-41ec-cf9f-718621556559"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/DHS-LLM-Workshop/personal_copilot/training\n"
          ]
        }
      ],
      "source": [
        "%cd personal_copilot/training/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN3ZNTQZ6kCP",
        "outputId": "4362c316-ec43-4ef7-a2ce-b8d495753daf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated git hooks.\n",
            "Git LFS initialized.\n"
          ]
        }
      ],
      "source": [
        "!git lfs install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqtjrruXXKu9",
        "outputId": "bea67f69-b154-42e3-eee9-b279873ffa0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already up to date.\n",
            "2023-08-19 15:06:46.403224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:631: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Downloading (…)okenizer_config.json: 100% 677/677 [00:00<00:00, 3.44MB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 777k/777k [00:00<00:00, 20.9MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 442k/442k [00:00<00:00, 57.4MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 2.06M/2.06M [00:00<00:00, 6.23MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 532/532 [00:00<00:00, 2.91MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'token=True' instead.\n",
            "  warnings.warn(\n",
            "Downloading readme: 100% 478/478 [00:00<00:00, 3.03MB/s]\n",
            "Downloading data files:   0% 0/1 [00:00<?, ?it/s]\n",
            "Downloading data:   0% 0.00/30.6M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:  14% 4.19M/30.6M [00:00<00:03, 8.64MB/s]\u001b[A\n",
            "Downloading data:  41% 12.6M/30.6M [00:00<00:00, 18.5MB/s]\u001b[A\n",
            "Downloading data:  69% 21.0M/30.6M [00:00<00:00, 24.4MB/s]\u001b[A\n",
            "Downloading data: 100% 30.6M/30.6M [00:01<00:00, 15.8MB/s]\n",
            "Downloading data files: 100% 1/1 [00:01<00:00,  1.94s/it]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 1439.86it/s]\n",
            "Setting num_proc from 4 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 100% 5905/5905 [00:00<00:00, 13544.72 examples/s]\n",
            "Size of the train set: 5875. Size of the validation set: 30\n",
            "100% 400/400 [00:05<00:00, 76.89it/s]\n",
            "The character to token ratio of the dataset is: 3.29\n",
            "Loading the model\n",
            "Downloading (…)lve/main/config.json: 100% 1.05k/1.05k [00:00<00:00, 5.50MB/s]\n",
            "Downloading model.safetensors: 100% 4.55G/4.55G [00:25<00:00, 175MB/s]\n",
            "Downloading (…)neration_config.json: 100% 111/111 [00:00<00:00, 617kB/s]\n",
            "GPTBigCodeForCausalLM(\n",
            "  (transformer): GPTBigCodeModel(\n",
            "    (wte): Embedding(49152, 2048)\n",
            "    (wpe): Embedding(8192, 2048)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-23): 24 x GPTBigCodeBlock(\n",
            "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPTBigCodeAttention(\n",
            "          (c_attn): Linear(in_features=2048, out_features=2304, bias=True)\n",
            "          (c_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPTBigCodeMLP(\n",
            "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
            "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
            "          (act): PytorchGELUTanh()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=49152, bias=False)\n",
            ")\n",
            "Starting main loop\n",
            "Training...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msmangrul\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DHS-LLM-Workshop/personal_copilot/training/wandb/run-20230819_150752-g26jvbtd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstarcoder-copilot\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/smangrul/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/smangrul/huggingface/runs/g26jvbtd\u001b[0m\n",
            "{'loss': 0.886, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.01}\n",
            "{'loss': 0.7542, 'learning_rate': 4.998728546500082e-05, 'epoch': 0.03}\n",
            "{'loss': 0.7734, 'learning_rate': 4.993565483078743e-05, 'epoch': 0.04}\n",
            "{'loss': 0.8203, 'learning_rate': 4.984439542929117e-05, 'epoch': 0.05}\n",
            "{'eval_loss': 0.800532341003418, 'eval_runtime': 1.5735, 'eval_samples_per_second': 31.141, 'eval_steps_per_second': 4.449, 'epoch': 0.05}\n",
            "{'loss': 0.7591, 'learning_rate': 4.971365229370284e-05, 'epoch': 0.06}\n",
            "{'loss': 0.8271, 'learning_rate': 4.9543633206385834e-05, 'epoch': 0.07}\n",
            "{'loss': 0.8693, 'learning_rate': 4.933460836865996e-05, 'epoch': 0.09}\n",
            "{'loss': 0.762, 'learning_rate': 4.9086909971386305e-05, 'epoch': 0.1}\n",
            "{'eval_loss': 0.6704105734825134, 'eval_runtime': 1.5749, 'eval_samples_per_second': 31.113, 'eval_steps_per_second': 4.445, 'epoch': 0.1}\n",
            "{'loss': 0.6666, 'learning_rate': 4.8800931667035946e-05, 'epoch': 0.11}\n",
            "{'loss': 0.6257, 'learning_rate': 4.847712794408124e-05, 'epoch': 0.12}\n",
            "{'loss': 0.6967, 'learning_rate': 4.8116013404704055e-05, 'epoch': 0.14}\n",
            "{'loss': 0.7456, 'learning_rate': 4.7718161946968835e-05, 'epoch': 0.15}\n",
            "{'eval_loss': 0.5798209309577942, 'eval_runtime': 1.5761, 'eval_samples_per_second': 31.089, 'eval_steps_per_second': 4.441, 'epoch': 0.15}\n",
            "{'loss': 0.7095, 'learning_rate': 4.728420585276026e-05, 'epoch': 0.16}\n",
            "{'loss': 0.7202, 'learning_rate': 4.6814834782934844e-05, 'epoch': 0.17}\n",
            "{'loss': 0.7091, 'learning_rate': 4.631079468128358e-05, 'epoch': 0.19}\n",
            "{'loss': 0.6643, 'learning_rate': 4.577288658904741e-05, 'epoch': 0.2}\n",
            "{'eval_loss': 0.541455864906311, 'eval_runtime': 1.5713, 'eval_samples_per_second': 31.185, 'eval_steps_per_second': 4.455, 'epoch': 0.2}\n",
            "{'loss': 0.6275, 'learning_rate': 4.5201965371869605e-05, 'epoch': 0.21}\n",
            "{'loss': 0.5764, 'learning_rate': 4.4598938361208095e-05, 'epoch': 0.23}\n",
            "{'loss': 0.6317, 'learning_rate': 4.396476391236707e-05, 'epoch': 0.24}\n",
            "{'loss': 0.6, 'learning_rate': 4.3300449881439375e-05, 'epoch': 0.25}\n",
            "{'eval_loss': 0.5265635848045349, 'eval_runtime': 1.5885, 'eval_samples_per_second': 30.846, 'eval_steps_per_second': 4.407, 'epoch': 0.25}\n",
            "{'loss': 0.6981, 'learning_rate': 4.260705202358014e-05, 'epoch': 0.26}\n",
            "{'loss': 0.7856, 'learning_rate': 4.1885672315157346e-05, 'epoch': 0.28}\n",
            "{'loss': 0.7855, 'learning_rate': 4.1137457202445665e-05, 'epoch': 0.29}\n",
            "{'loss': 0.5847, 'learning_rate': 4.0363595779647e-05, 'epoch': 0.3}\n",
            "{'eval_loss': 0.4981347322463989, 'eval_runtime': 1.58, 'eval_samples_per_second': 31.013, 'eval_steps_per_second': 4.43, 'epoch': 0.3}\n",
            "{'loss': 0.5492, 'learning_rate': 3.956531789913301e-05, 'epoch': 0.31}\n",
            "{'loss': 0.5142, 'learning_rate': 3.874389221691329e-05, 'epoch': 0.33}\n",
            "{'loss': 0.5126, 'learning_rate': 3.7900624176435066e-05, 'epoch': 0.34}\n",
            "{'loss': 0.5323, 'learning_rate': 3.7036853933918784e-05, 'epoch': 0.35}\n",
            "{'eval_loss': 0.4615928530693054, 'eval_runtime': 1.578, 'eval_samples_per_second': 31.051, 'eval_steps_per_second': 4.436, 'epoch': 0.35}\n",
            "{'loss': 1.0308, 'learning_rate': 3.6153954228526894e-05, 'epoch': 0.36}\n",
            "{'loss': 0.9682, 'learning_rate': 3.5253328200750224e-05, 'epoch': 0.38}\n",
            "{'loss': 0.8723, 'learning_rate': 3.433640716247956e-05, 'epoch': 0.39}\n",
            "{'loss': 0.608, 'learning_rate': 3.340464832230592e-05, 'epoch': 0.4}\n",
            "{'eval_loss': 0.45269253849983215, 'eval_runtime': 1.5766, 'eval_samples_per_second': 31.079, 'eval_steps_per_second': 4.44, 'epoch': 0.4}\n",
            "{'loss': 0.5695, 'learning_rate': 3.245953246966484e-05, 'epoch': 0.41}\n",
            "{'loss': 0.4943, 'learning_rate': 3.1502561621504874e-05, 'epoch': 0.42}\n",
            "{'loss': 0.46, 'learning_rate': 3.053525663522062e-05, 'epoch': 0.44}\n",
            "{'loss': 0.4632, 'learning_rate': 2.95591547916436e-05, 'epoch': 0.45}\n",
            "{'eval_loss': 0.4479658901691437, 'eval_runtime': 1.5816, 'eval_samples_per_second': 30.982, 'eval_steps_per_second': 4.426, 'epoch': 0.45}\n",
            "{'loss': 0.5269, 'learning_rate': 2.8575807351932426e-05, 'epoch': 0.46}\n",
            "{'loss': 0.4745, 'learning_rate': 2.7586777092244804e-05, 'epoch': 0.47}\n",
            "{'loss': 0.5375, 'learning_rate': 2.659363582010948e-05, 'epoch': 0.49}\n",
            "{'loss': 0.5385, 'learning_rate': 2.5597961876445077e-05, 'epoch': 0.5}\n",
            "{'eval_loss': 0.42852044105529785, 'eval_runtime': 1.5849, 'eval_samples_per_second': 30.917, 'eval_steps_per_second': 4.417, 'epoch': 0.5}\n",
            "{'loss': 0.57, 'learning_rate': 2.46013376271959e-05, 'epoch': 0.51}\n",
            "{'loss': 0.4137, 'learning_rate': 2.360534694857094e-05, 'epoch': 0.53}\n",
            "{'loss': 0.4091, 'learning_rate': 2.2611572709882706e-05, 'epoch': 0.54}\n",
            "{'loss': 0.4835, 'learning_rate': 2.162159425798629e-05, 'epoch': 0.55}\n",
            "{'eval_loss': 0.4098097085952759, 'eval_runtime': 1.5793, 'eval_samples_per_second': 31.026, 'eval_steps_per_second': 4.432, 'epoch': 0.55}\n",
            "{'loss': 0.5565, 'learning_rate': 2.0636984907316447e-05, 'epoch': 0.56}\n",
            "{'loss': 0.5271, 'learning_rate': 1.9659309439511628e-05, 'epoch': 0.57}\n",
            "{'loss': 0.4863, 'learning_rate': 1.8690121616598745e-05, 'epoch': 0.59}\n",
            "{'loss': 0.5149, 'learning_rate': 1.7730961711690655e-05, 'epoch': 0.6}\n",
            "{'eval_loss': 0.3829399049282074, 'eval_runtime': 1.5916, 'eval_samples_per_second': 30.787, 'eval_steps_per_second': 4.398, 'epoch': 0.6}\n",
            "{'loss': 0.495, 'learning_rate': 1.6783354061120878e-05, 'epoch': 0.61}\n",
            "{'loss': 0.4129, 'learning_rate': 1.5848804641905634e-05, 'epoch': 0.62}\n",
            "{'loss': 0.3991, 'learning_rate': 1.4928798678383337e-05, 'epoch': 0.64}\n",
            "{'loss': 0.4233, 'learning_rate': 1.4024798281834966e-05, 'epoch': 0.65}\n",
            "{'eval_loss': 0.376846581697464, 'eval_runtime': 1.5754, 'eval_samples_per_second': 31.103, 'eval_steps_per_second': 4.443, 'epoch': 0.65}\n",
            "{'loss': 0.4374, 'learning_rate': 1.3138240126836831e-05, 'epoch': 0.66}\n",
            "{'loss': 0.4768, 'learning_rate': 1.2270533168038217e-05, 'epoch': 0.68}\n",
            "{'loss': 0.5955, 'learning_rate': 1.1423056400992816e-05, 'epoch': 0.69}\n",
            "{'loss': 0.5448, 'learning_rate': 1.0597156670602299e-05, 'epoch': 0.7}\n",
            "{'eval_loss': 0.36956071853637695, 'eval_runtime': 1.5762, 'eval_samples_per_second': 31.088, 'eval_steps_per_second': 4.441, 'epoch': 0.7}\n",
            "{'loss': 0.4893, 'learning_rate': 9.794146530655082e-06, 'epoch': 0.71}\n",
            "{'loss': 0.3544, 'learning_rate': 9.015302157861883e-06, 'epoch': 0.72}\n",
            "{'loss': 0.3399, 'learning_rate': 8.261861323703288e-06, 'epoch': 0.74}\n",
            "{'loss': 0.3498, 'learning_rate': 7.535021427312417e-06, 'epoch': 0.75}\n",
            "{'eval_loss': 0.3662794828414917, 'eval_runtime': 1.584, 'eval_samples_per_second': 30.935, 'eval_steps_per_second': 4.419, 'epoch': 0.75}\n",
            "{'loss': 0.3287, 'learning_rate': 6.835937592519031e-06, 'epoch': 0.76}\n",
            "{'loss': 0.5849, 'learning_rate': 6.1657208320793054e-06, 'epoch': 0.78}\n",
            "{'loss': 0.7833, 'learning_rate': 5.525436282008664e-06, 'epoch': 0.79}\n",
            "{'loss': 0.7823, 'learning_rate': 4.916101508823873e-06, 'epoch': 0.8}\n",
            "{'eval_loss': 0.36369919776916504, 'eval_runtime': 1.5797, 'eval_samples_per_second': 31.019, 'eval_steps_per_second': 4.431, 'epoch': 0.8}\n",
            "{'loss': 0.4624, 'learning_rate': 4.33868489238452e-06, 'epoch': 0.81}\n",
            "{'loss': 0.4357, 'learning_rate': 3.7941040869039714e-06, 'epoch': 0.82}\n",
            "{'loss': 0.3876, 'learning_rate': 3.283224562575543e-06, 'epoch': 0.84}\n",
            "{'loss': 0.3285, 'learning_rate': 2.8068582301317425e-06, 'epoch': 0.85}\n",
            "{'eval_loss': 0.3618001341819763, 'eval_runtime': 1.5821, 'eval_samples_per_second': 30.971, 'eval_steps_per_second': 4.424, 'epoch': 0.85}\n",
            "{'loss': 0.3239, 'learning_rate': 2.3657621505223576e-06, 'epoch': 0.86}\n",
            "{'loss': 0.3545, 'learning_rate': 1.960637331762091e-06, 'epoch': 0.88}\n",
            "{'loss': 0.3925, 'learning_rate': 1.592127614859812e-06, 'epoch': 0.89}\n",
            "{'loss': 0.3635, 'learning_rate': 1.2608186505999847e-06, 'epoch': 0.9}\n",
            "{'eval_loss': 0.3636470437049866, 'eval_runtime': 1.5744, 'eval_samples_per_second': 31.123, 'eval_steps_per_second': 4.446, 'epoch': 0.9}\n",
            "{'loss': 0.4415, 'learning_rate': 9.672369688023542e-07, 'epoch': 0.91}\n",
            "{'loss': 0.4382, 'learning_rate': 7.118491415391337e-07, 'epoch': 0.93}\n",
            "{'loss': 0.3438, 'learning_rate': 4.950610416395052e-07, 'epoch': 0.94}\n",
            "{'loss': 0.3066, 'learning_rate': 3.1721719765981926e-07, 'epoch': 0.95}\n",
            "{'eval_loss': 0.3630111813545227, 'eval_runtime': 1.5882, 'eval_samples_per_second': 30.853, 'eval_steps_per_second': 4.408, 'epoch': 0.95}\n",
            "{'loss': 0.3055, 'learning_rate': 1.7860024634467064e-07, 'epoch': 0.96}\n",
            "{'loss': 0.4371, 'learning_rate': 7.94304834489723e-08, 'epoch': 0.97}\n",
            "{'loss': 0.504, 'learning_rate': 1.9865513634884094e-08, 'epoch': 0.99}\n",
            "{'loss': 0.4495, 'learning_rate': 0.0, 'epoch': 1.0}\n",
            "{'eval_loss': 0.362487256526947, 'eval_runtime': 1.5772, 'eval_samples_per_second': 31.068, 'eval_steps_per_second': 4.438, 'epoch': 1.0}\n",
            "{'train_runtime': 4225.211, 'train_samples_per_second': 7.574, 'train_steps_per_second': 0.473, 'train_loss': 0.5620492153167724, 'epoch': 1.0}\n",
            "100% 2000/2000 [1:10:23<00:00,  2.11s/it]\n",
            "Saving last checkpoint of the model\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▆▄▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ▂▂▃▁▇▄▃▃▅▆▄█▂▃▅▄▅▂▇▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▇▇▆█▂▅▆▆▄▃▅▁▇▆▄▅▄▇▂▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▇▇▆█▂▅▆▆▄▃▅▁▇▆▄▅▄▇▂▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ▇██████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ▇▆▅▆▄▅▅▅▄▄▅▆▃▃█▆▄▂▃▃▄▂▃▃▃▂▂▄▃▁▁▆▃▂▁▂▂▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.36249\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 1.5772\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 31.068\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 4.438\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 2000\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.4495\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 4.00988615737344e+17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.56205\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 4225.211\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 7.574\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 0.473\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mstarcoder-copilot\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/smangrul/huggingface/runs/g26jvbtd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/smangrul/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjg4MDk3MDM4/version_details/v9\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230819_150752-g26jvbtd/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!git pull\n",
        "!python train.py \\\n",
        "    --model_path \"bigcode/starcoderbase-1b\" \\\n",
        "    --dataset_name \"smangrul/hf-stack-v1\" \\\n",
        "    --subset \"data\" \\\n",
        "    --data_column \"content\" \\\n",
        "    --split \"train\" \\\n",
        "    --seq_length 2048 \\\n",
        "    --max_steps 2000 \\\n",
        "    --batch_size 8 \\\n",
        "    --gradient_accumulation_steps 2 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --lr_scheduler_type \"cosine\" \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --num_warmup_steps 30 \\\n",
        "    --eval_freq 100 \\\n",
        "    --save_freq 500 \\\n",
        "    --log_freq 25 \\\n",
        "    --num_workers 4 \\\n",
        "    --bf16 \\\n",
        "    --no_fp16 \\\n",
        "    --output_dir \"starcoderbase1b-personal-copilot-A100-40GB-colab\" \\\n",
        "    --fim_rate 0.5 \\\n",
        "    --fim_spm_rate 0.5 \\\n",
        "    --use_flash_attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hp1_Gw4VXuQR",
        "outputId": "beb77149-080c-431a-8b3c-e8c164d4cd2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already up to date.\n",
            "2023-08-19 16:33:57.400986: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:631: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'token=True' instead.\n",
            "  warnings.warn(\n",
            "Size of the train set: 5875. Size of the validation set: 30\n",
            "100% 400/400 [00:04<00:00, 85.01it/s]\n",
            "The character to token ratio of the dataset is: 3.29\n",
            "Loading the model\n",
            "PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPTBigCodeForCausalLM(\n",
            "      (transformer): GPTBigCodeModel(\n",
            "        (wte): Embedding(49152, 2048)\n",
            "        (wpe): Embedding(8192, 2048)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-23): 24 x GPTBigCodeBlock(\n",
            "            (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPTBigCodeAttention(\n",
            "              (c_attn): Linear(\n",
            "                in_features=2048, out_features=2304, bias=True\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=64, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (c_proj): Linear(\n",
            "                in_features=2048, out_features=2048, bias=True\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPTBigCodeMLP(\n",
            "              (c_fc): Linear(\n",
            "                in_features=2048, out_features=8192, bias=True\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=64, out_features=8192, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (c_proj): Linear(\n",
            "                in_features=8192, out_features=2048, bias=True\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=8192, out_features=64, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (act): PytorchGELUTanh()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=2048, out_features=49152, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Starting main loop\n",
            "Training...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msmangrul\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DHS-LLM-Workshop/personal_copilot/training/wandb/run-20230819_163459-xerzmupt\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstarcoder-copilot\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/smangrul/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/smangrul/huggingface/runs/xerzmupt\u001b[0m\n",
            "{'loss': 0.8668, 'learning_rate': 0.0004166666666666667, 'epoch': 0.01}\n",
            "{'loss': 0.8603, 'learning_rate': 0.0004998728546500082, 'epoch': 0.03}\n",
            "{'loss': 0.8671, 'learning_rate': 0.0004993565483078742, 'epoch': 0.04}\n",
            "{'loss': 0.8697, 'learning_rate': 0.0004984439542929117, 'epoch': 0.05}\n",
            "{'eval_loss': 0.7531298398971558, 'eval_runtime': 2.3632, 'eval_samples_per_second': 20.735, 'eval_steps_per_second': 1.693, 'epoch': 0.05}\n",
            "{'loss': 0.7088, 'learning_rate': 0.0004971365229370283, 'epoch': 0.06}\n",
            "{'loss': 0.779, 'learning_rate': 0.0004954363320638583, 'epoch': 0.07}\n",
            "{'loss': 0.7701, 'learning_rate': 0.0004933460836865995, 'epoch': 0.09}\n",
            "{'loss': 0.7477, 'learning_rate': 0.000490869099713863, 'epoch': 0.1}\n",
            "{'eval_loss': 0.6309488415718079, 'eval_runtime': 2.3763, 'eval_samples_per_second': 20.62, 'eval_steps_per_second': 1.683, 'epoch': 0.1}\n",
            "{'loss': 0.6832, 'learning_rate': 0.00048800931667035945, 'epoch': 0.11}\n",
            "{'loss': 0.6568, 'learning_rate': 0.0004847712794408124, 'epoch': 0.12}\n",
            "{'loss': 0.8089, 'learning_rate': 0.0004811601340470405, 'epoch': 0.14}\n",
            "{'loss': 0.7431, 'learning_rate': 0.00047718161946968835, 'epoch': 0.15}\n",
            "{'eval_loss': 0.6067564487457275, 'eval_runtime': 2.3696, 'eval_samples_per_second': 20.678, 'eval_steps_per_second': 1.688, 'epoch': 0.15}\n",
            "{'loss': 0.6228, 'learning_rate': 0.00047284205852760264, 'epoch': 0.16}\n",
            "{'loss': 0.5973, 'learning_rate': 0.00046814834782934843, 'epoch': 0.17}\n",
            "{'loss': 0.9981, 'learning_rate': 0.0004631079468128358, 'epoch': 0.19}\n",
            "{'loss': 0.8674, 'learning_rate': 0.00045772886589047414, 'epoch': 0.2}\n",
            "{'eval_loss': 0.5779393315315247, 'eval_runtime': 2.3693, 'eval_samples_per_second': 20.681, 'eval_steps_per_second': 1.688, 'epoch': 0.2}\n",
            "{'loss': 0.6486, 'learning_rate': 0.00045201965371869605, 'epoch': 0.21}\n",
            "{'loss': 0.5876, 'learning_rate': 0.0004459893836120809, 'epoch': 0.23}\n",
            "{'loss': 0.6296, 'learning_rate': 0.0004396476391236707, 'epoch': 0.24}\n",
            "{'loss': 0.7061, 'learning_rate': 0.0004330044988143937, 'epoch': 0.25}\n",
            "{'eval_loss': 0.5563331842422485, 'eval_runtime': 2.3699, 'eval_samples_per_second': 20.676, 'eval_steps_per_second': 1.688, 'epoch': 0.25}\n",
            "{'loss': 0.6414, 'learning_rate': 0.0004260705202358014, 'epoch': 0.26}\n",
            "{'loss': 0.5558, 'learning_rate': 0.0004188567231515734, 'epoch': 0.28}\n",
            "{'loss': 0.6689, 'learning_rate': 0.00041137457202445666, 'epoch': 0.29}\n",
            "{'loss': 0.6532, 'learning_rate': 0.00040363595779646996, 'epoch': 0.3}\n",
            "{'eval_loss': 0.501397430896759, 'eval_runtime': 2.3715, 'eval_samples_per_second': 20.662, 'eval_steps_per_second': 1.687, 'epoch': 0.3}\n",
            "{'loss': 0.5849, 'learning_rate': 0.0003956531789913301, 'epoch': 0.31}\n",
            "{'loss': 0.5497, 'learning_rate': 0.0003874389221691329, 'epoch': 0.33}\n",
            "{'loss': 0.5868, 'learning_rate': 0.0003790062417643506, 'epoch': 0.34}\n",
            "{'loss': 0.7218, 'learning_rate': 0.00037036853933918783, 'epoch': 0.35}\n",
            "{'eval_loss': 0.4968039095401764, 'eval_runtime': 2.3812, 'eval_samples_per_second': 20.578, 'eval_steps_per_second': 1.68, 'epoch': 0.35}\n",
            "{'loss': 0.5589, 'learning_rate': 0.0003615395422852689, 'epoch': 0.36}\n",
            "{'loss': 0.5083, 'learning_rate': 0.00035253328200750224, 'epoch': 0.38}\n",
            "{'loss': 0.6096, 'learning_rate': 0.00034336407162479553, 'epoch': 0.39}\n",
            "{'loss': 0.9061, 'learning_rate': 0.0003340464832230592, 'epoch': 0.4}\n",
            "{'eval_loss': 0.4863089621067047, 'eval_runtime': 2.3728, 'eval_samples_per_second': 20.651, 'eval_steps_per_second': 1.686, 'epoch': 0.4}\n",
            "{'loss': 0.6039, 'learning_rate': 0.00032459532469664837, 'epoch': 0.41}\n",
            "{'loss': 0.5285, 'learning_rate': 0.00031502561621504874, 'epoch': 0.42}\n",
            "{'loss': 0.4986, 'learning_rate': 0.0003053525663522062, 'epoch': 0.44}\n",
            "{'loss': 0.5569, 'learning_rate': 0.000295591547916436, 'epoch': 0.45}\n",
            "{'eval_loss': 0.4807043671607971, 'eval_runtime': 2.3767, 'eval_samples_per_second': 20.617, 'eval_steps_per_second': 1.683, 'epoch': 0.45}\n",
            "{'loss': 0.6426, 'learning_rate': 0.00028575807351932425, 'epoch': 0.46}\n",
            "{'loss': 0.4833, 'learning_rate': 0.000275867770922448, 'epoch': 0.47}\n",
            "{'loss': 0.55, 'learning_rate': 0.0002659363582010948, 'epoch': 0.49}\n",
            "{'loss': 0.5734, 'learning_rate': 0.00025597961876445074, 'epoch': 0.5}\n",
            "{'eval_loss': 0.44182345271110535, 'eval_runtime': 2.3629, 'eval_samples_per_second': 20.737, 'eval_steps_per_second': 1.693, 'epoch': 0.5}\n",
            "{'loss': 0.5667, 'learning_rate': 0.000246013376271959, 'epoch': 0.51}\n",
            "{'loss': 0.4872, 'learning_rate': 0.00023605346948570938, 'epoch': 0.53}\n",
            "{'loss': 0.4846, 'learning_rate': 0.00022611572709882704, 'epoch': 0.54}\n",
            "{'loss': 0.5504, 'learning_rate': 0.0002162159425798629, 'epoch': 0.55}\n",
            "{'eval_loss': 0.4402797520160675, 'eval_runtime': 2.3685, 'eval_samples_per_second': 20.688, 'eval_steps_per_second': 1.689, 'epoch': 0.55}\n",
            "{'loss': 0.6409, 'learning_rate': 0.00020636984907316448, 'epoch': 0.56}\n",
            "{'loss': 0.4321, 'learning_rate': 0.00019659309439511627, 'epoch': 0.57}\n",
            "{'loss': 0.4144, 'learning_rate': 0.00018690121616598743, 'epoch': 0.59}\n",
            "{'loss': 0.6654, 'learning_rate': 0.00017730961711690655, 'epoch': 0.6}\n",
            "{'eval_loss': 0.4332946836948395, 'eval_runtime': 2.3804, 'eval_samples_per_second': 20.585, 'eval_steps_per_second': 1.68, 'epoch': 0.6}\n",
            "{'loss': 0.7837, 'learning_rate': 0.00016783354061120876, 'epoch': 0.61}\n",
            "{'loss': 0.4984, 'learning_rate': 0.00015848804641905635, 'epoch': 0.62}\n",
            "{'loss': 0.4385, 'learning_rate': 0.00014928798678383337, 'epoch': 0.64}\n",
            "{'loss': 0.4556, 'learning_rate': 0.00014024798281834965, 'epoch': 0.65}\n",
            "{'eval_loss': 0.4310839772224426, 'eval_runtime': 2.377, 'eval_samples_per_second': 20.614, 'eval_steps_per_second': 1.683, 'epoch': 0.65}\n",
            "{'loss': 0.5309, 'learning_rate': 0.00013138240126836832, 'epoch': 0.66}\n",
            "{'loss': 0.552, 'learning_rate': 0.00012270533168038216, 'epoch': 0.68}\n",
            "{'loss': 0.3872, 'learning_rate': 0.00011423056400992815, 'epoch': 0.69}\n",
            "{'loss': 0.5476, 'learning_rate': 0.00010597156670602298, 'epoch': 0.7}\n",
            "{'eval_loss': 0.4204583168029785, 'eval_runtime': 2.3698, 'eval_samples_per_second': 20.677, 'eval_steps_per_second': 1.688, 'epoch': 0.7}\n",
            "{'loss': 0.5351, 'learning_rate': 9.794146530655082e-05, 'epoch': 0.71}\n",
            "{'loss': 0.491, 'learning_rate': 9.015302157861882e-05, 'epoch': 0.72}\n",
            "{'loss': 0.4255, 'learning_rate': 8.261861323703288e-05, 'epoch': 0.74}\n",
            "{'loss': 0.4514, 'learning_rate': 7.535021427312416e-05, 'epoch': 0.75}\n",
            "{'eval_loss': 0.4161112606525421, 'eval_runtime': 2.3624, 'eval_samples_per_second': 20.742, 'eval_steps_per_second': 1.693, 'epoch': 0.75}\n",
            "{'loss': 0.5417, 'learning_rate': 6.835937592519031e-05, 'epoch': 0.76}\n",
            "{'loss': 0.5366, 'learning_rate': 6.165720832079305e-05, 'epoch': 0.78}\n",
            "{'loss': 0.38, 'learning_rate': 5.525436282008664e-05, 'epoch': 0.79}\n",
            "{'loss': 0.3709, 'learning_rate': 4.916101508823872e-05, 'epoch': 0.8}\n",
            "{'eval_loss': 0.4150681793689728, 'eval_runtime': 2.374, 'eval_samples_per_second': 20.641, 'eval_steps_per_second': 1.685, 'epoch': 0.8}\n",
            "{'loss': 0.7516, 'learning_rate': 4.33868489238452e-05, 'epoch': 0.81}\n",
            "{'loss': 0.6582, 'learning_rate': 3.7941040869039715e-05, 'epoch': 0.82}\n",
            "{'loss': 0.4356, 'learning_rate': 3.283224562575543e-05, 'epoch': 0.84}\n",
            "{'loss': 0.4068, 'learning_rate': 2.8068582301317424e-05, 'epoch': 0.85}\n",
            "{'eval_loss': 0.4148676097393036, 'eval_runtime': 2.3643, 'eval_samples_per_second': 20.725, 'eval_steps_per_second': 1.692, 'epoch': 0.85}\n",
            "{'loss': 0.4446, 'learning_rate': 2.3657621505223575e-05, 'epoch': 0.86}\n",
            "{'loss': 0.5087, 'learning_rate': 1.960637331762091e-05, 'epoch': 0.88}\n",
            "{'loss': 0.4927, 'learning_rate': 1.5921276148598118e-05, 'epoch': 0.89}\n",
            "{'loss': 0.3977, 'learning_rate': 1.2608186505999846e-05, 'epoch': 0.9}\n",
            "{'eval_loss': 0.4162611961364746, 'eval_runtime': 2.3609, 'eval_samples_per_second': 20.754, 'eval_steps_per_second': 1.694, 'epoch': 0.9}\n",
            "{'loss': 0.5269, 'learning_rate': 9.67236968802354e-06, 'epoch': 0.91}\n",
            "{'loss': 0.5127, 'learning_rate': 7.118491415391337e-06, 'epoch': 0.93}\n",
            "{'loss': 0.4706, 'learning_rate': 4.950610416395051e-06, 'epoch': 0.94}\n",
            "{'loss': 0.4122, 'learning_rate': 3.1721719765981927e-06, 'epoch': 0.95}\n",
            "{'eval_loss': 0.4155018925666809, 'eval_runtime': 2.3598, 'eval_samples_per_second': 20.765, 'eval_steps_per_second': 1.695, 'epoch': 0.95}\n",
            "{'loss': 0.4427, 'learning_rate': 1.7860024634467064e-06, 'epoch': 0.96}\n",
            "{'loss': 0.59, 'learning_rate': 7.94304834489723e-07, 'epoch': 0.97}\n",
            "{'loss': 0.4524, 'learning_rate': 1.986551363488409e-07, 'epoch': 0.99}\n",
            "{'loss': 0.3718, 'learning_rate': 0.0, 'epoch': 1.0}\n",
            "{'eval_loss': 0.41535162925720215, 'eval_runtime': 2.3686, 'eval_samples_per_second': 20.688, 'eval_steps_per_second': 1.689, 'epoch': 1.0}\n",
            "{'train_runtime': 10949.6367, 'train_samples_per_second': 5.845, 'train_steps_per_second': 0.183, 'train_loss': 0.5880520067214966, 'epoch': 1.0}\n",
            "100% 2000/2000 [3:02:27<00:00,  5.47s/it]\n",
            "Saving last checkpoint of the model\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▅▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ▂▆▄▄▄▅█▅▇▂▄█▇▄▂▆▂▁▁▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▇▃▅▅▅▄▁▄▂▇▅▁▂▅▇▃▇██▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▇▂▅▅▅▄▁▄▂▇▅▁▂▅▇▃▇██▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ▇██████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ▇▇▅▅▄▆▄█▄▄▄▄▃▃▃▄▄▂▄▃▃▂▄▁▆▂▃▁▃▂▃▁▅▂▂▂▃▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.41535\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 2.3686\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 20.688\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 1.689\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 2000\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.3718\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 8.36921085394944e+17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.58805\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 10949.6367\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 5.845\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 0.183\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mstarcoder-copilot\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/smangrul/huggingface/runs/xerzmupt\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/smangrul/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjg4MDk3MDM4/version_details/v9\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230819_163459-xerzmupt/logs\u001b[0m\n"
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "076bcade218645258f75c862699dcf66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19c5bab192004bd4a3cb068a1c7fd8ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d1551729b8b4a6b9d07e3fe52bb3447": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b25545fe76fc41988dc7908b366a7214",
            "placeholder": "​",
            "style": "IPY_MODEL_963d4746ccd84d24be0a7928ef70fea3",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "25f80e20899f42b89523a7a12f5ae3ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2631660841724a7aa1281c91af705267": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "3098dd6d5bbc48c5ba618de25cd53ce0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3399240402c2453494d8378dfca965e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19c5bab192004bd4a3cb068a1c7fd8ea",
            "placeholder": "​",
            "style": "IPY_MODEL_25f80e20899f42b89523a7a12f5ae3ea",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "6bf0c013227e4aaaa8ec4eb93eeaf845": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b3f10fd6c804e4981fdef7072c892bc",
            "placeholder": "​",
            "style": "IPY_MODEL_3098dd6d5bbc48c5ba618de25cd53ce0",
            "value": "Token is valid (permission: write)."
          }
        },
        "963d4746ccd84d24be0a7928ef70fea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b3f10fd6c804e4981fdef7072c892bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b25545fe76fc41988dc7908b366a7214": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf79f76aa37549cd91637532813750ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff012677cb634ed5afdcb6a864af172b",
            "placeholder": "​",
            "style": "IPY_MODEL_076bcade218645258f75c862699dcf66",
            "value": "Login successful"
          }
        },
        "dcbe7f239f7c43f7a894ffe6f7abd54f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6bf0c013227e4aaaa8ec4eb93eeaf845",
              "IPY_MODEL_1d1551729b8b4a6b9d07e3fe52bb3447",
              "IPY_MODEL_3399240402c2453494d8378dfca965e7",
              "IPY_MODEL_bf79f76aa37549cd91637532813750ee"
            ],
            "layout": "IPY_MODEL_2631660841724a7aa1281c91af705267"
          }
        },
        "ff012677cb634ed5afdcb6a864af172b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}